Neural networks existed since Rosenblatt presented his work in XXXX, there were even training algorithms with the perceptron.
Cold water was poured on the field when Minsky published Perceptrons which explained that the type of NNs fell into two categories, single-layer Perceotrons and multi-layer Perceptrons.
While SLPs could be trained, they couldn't solve all problems.
While MLPs could solve any function to an arbitrary accuracy, they couldn't be trained automatically.
MLPs could be handcrafted to perform any activity, but this was akin to programming an algorithm.
Eventually a method known as Backpropovatio was presented which coukd, and ANNs found some successes, but either the software or hardware wasn't enough, even though techbically they shoukd have been strong.
Deep learning came along once hardware had improved, and graphics cards were a thing, and so rejuvinated interest.

Deep learning is different to BP, but only in a small sense. it works layer by layer, allowing for bigger networks.
