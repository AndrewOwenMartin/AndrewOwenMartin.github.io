# AI
# if NFL is true, there can not be GAI.

# The primary sense is 'feel'

Vision is not what you think it is. What you are experiencing is 'feel', vision is a component of that but no more important than history and situational knowledge.

When you're looking around you can see that you're in a street or that you're in a room. If you close your eyes that experience is drastically different, for sure, but pay close attention to the "feelings" that remain. Of you are still likely to "feel" that you are in the street or a room. If you were feeling bored or stressed with your eyes open you'll probably still feel bored or stressed with your eyes closed.

It is sometimes said that vision is the primary sense, it seems to be the most important and first way we learn about what's going on. It is also said (I've heard it from a community which was either about deafness or blindness, I forget) hearing is the social sense.

A neuroscientist said that smell is the primary sense. Defining smell as the sense where soluable chemicals contact the cell membrane. Even touch, oressure sensitivity cones later evolutionarily. Maybe metabolism, but even that assmes soluable chemicals.

# Your own mind is doing the heavy lifting

If you see an image that claims to he the output of some computer vision system remember that what you're experiexperiencing is the result of your own act of perception and experience on top of whatever the computer output.
It's easy to see the output of an edge detector and say to yourself "yeah, that looks like the edges are highlighted in white" but then you're doing your own "edge detection" and checking that the result is more or less plausible.
The real output of an edge detector will look like a list, an easily interpretable JSON object, a binary string that in a conventional format for listing edges, or ultimately some intelligent behavior.

# context in AI is like management.

A mangement structure starts of ad-hoc, in a system where everyone can speak to everyone else and get a feel for their situation, just like a shared lived experience.
As the company grows there will be fewer connections between the people, and so information needs to get passed in a more fomalised manner.
This has the advantage of ignoring irellevant details, but disadvantage of losing the shared lived-experience.
This isn't so bad if the person who is reporting "up the chain" has freedom to report what they want as they can use their own knowledge and experience to decide which details are relevant on an ad-hoc basis.
This is a problem if the upper manager has dicitated what the model of communication must be. E.g. report on these metrics, ignore everything else.

A formally defined communication is efficient but necessarily loses information. This is great when it is known beforehand which information is relevant.

So, as a company grows you run the risk of the lower workers feeling like they are inhumanely treated and simply reaching metrics.
One solution to this is to give each area its own autonomy and enforce that some upper management cannot dictate on certain issues without having appropriate experience.
You end up with something like a distributed system of autonomous (as autonomous as anything can be) agents.
This is probably anathema to modern business, but may lead to more sustainable growth.

# goodhart's law

All AI risks turning a metric into a goal.
This is fine so long as the metric captures the entire context.
Otherwise you will have artificial maximising of an output, not an outcome.
Especially common is to tune an AI to a context in which the AI itself doesn't exist.
> What is a good example of this? 
> Something where the existance of the AI changes behaviour. Like the random skinner box.
> Maybe votes, encouraging popularism?

# there is no true autonomy

You may draw a line which defines autonomy, or an agent, in a system, but there is no objective way to draw the one true line.
